<html>
<head>
<style type="text/css">
p, pre
{
display:none;
 color:white;
  font-size:20px;
}

button
{
  height:50px;
  width:50px;
 }
</style>
<script type="text/javascript">
function copyToClipboard(element) {
  var $temp = $("<input>");
  $("body").append($temp);
  $temp.val($(element).text()).select();
  document.execCommand("copy");
  $temp.remove();
}
</script>
</head>
<body>
<link href='https://fonts.googleapis.com/css?family=Oswald' rel='stylesheet' type='text/css'> 

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>


<button onclick="copyToClipboard('#p1')">A</button>

<button onclick="copyToClipboard('#p2')">D</button>

<button onclick="copyToClipboard('#p3')">F</button>

<button onclick="copyToClipboard('#p4')">K</button>

<button onclick="copyToClipboard('#p5')">N</button>
 

       
<center>

<!--========================== APRIORI ======================== -->
<pre id="p1">
import collections
#!/usr/bin/env python
#The dict {} in python such as:my_information = {'name': 'Pusheen the Cat', 'country': 'USA', 'favorite_numbers': [42, 105]}
#name -> Pusheen the Cat (Key is name , value is Pusheen the cat)
import collections
def Apriori_gen(Itemset, lenght):
    """Too generate new (k+1)-itemsets can see README Join Stage"""
    canditate = []
    for i in range (0,lenght):
        element = str(Itemset[i])
        for j in range (i+1,lenght):
            element1 = str(Itemset[j])
            if element[0:(len(element)-1)] == element1[0:(len(element1)-1)]:
                    unionset = element[0:(len(element)-1)]+element1[len(element1)-1]+element[len(element)-1] #Combine (k-1)-Itemset to k-Itemset 
                    unionset = ''.join(sorted(unionset))  #Sort itemset by dict order
                    canditate.append(unionset)
    return canditate

def Apriori_prune(Ck,MinSupport):
    L = []
    for i in Ck:
        if Ck[i] >= minsupport:
            L.append(i)
    return sorted(L)

def Apriori_count_subset(Canditate,Canditate_len):
    """ Use bool to know is subset or not """
    Lk = dict()
    file = open('./dataset/apriori.csv')
    for l in file:
        l = str(l.split())
        for i in range (0,Canditate_len):
            key = str(Canditate[i])
            if key not in Lk:
                Lk[key] = 0
            flag = True
            for k in key:
                if k not in l:
                    flag = False
            if flag:
                Lk[key] += 1
    file.close()
    return Lk

print('Enter the min support price')
k=input()
minsupport = int(k)
print("Min Support is {}".format(minsupport))
C1={} 
file = open('./dataset/apriori.csv')
"""Count one canditate"""
for line in file:
    print(line)
    for item in line.split():
        if item in C1:
            C1[item] +=1
        else:
            C1[item] = 1
file.close()
C1 = collections.OrderedDict(sorted(C1.items()))
L = []
L1 = Apriori_prune(C1,minsupport)
print('====================================')
print('Frequent 1-itemset is',L1)
print('====================================')
L = Apriori_gen(L1,len(L1))
print('Candidate 1-set is',L)
k=2
while L != []:
    C = dict()
    C = Apriori_count_subset(L,len(L))
    fruquent_itemset = []
    fruquent_itemset = Apriori_prune(C,minsupport)
    print('====================================')
    print('Frequent',k,'-itemset is',fruquent_itemset)
    print('====================================')
    L = Apriori_gen(fruquent_itemset,len(fruquent_itemset))
    if L!=[]:
        print('Candidate',k,'-set is',L)
    k += 1
</pre>
 
 
 <!--========================== DECISION TREE ======================== -->
<pre id="p2">
import pandas as pd
import numpy as np
from pprint import pprint
dataset = pd.read_csv('dataset/decision_tree.csv',names=['outlook','temp','humidity','windy','play_golf',])
def entropy(target_col):
    elements,counts = np.unique(target_col,return_counts = True)
    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])
    return entropy
def InfoGain(data,split_attribute_name,target_name="play_golf"):   
    total_entropy = entropy(data[target_name]) 
    vals,counts= np.unique(data[split_attribute_name],return_counts=True)
    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])
    Information_Gain = total_entropy - Weighted_Entropy
    return Information_Gain       
def ID3(data,originaldata,features,target_attribute_name="play_golf",parent_node_class = None):
    if len(np.unique(data[target_attribute_name])) == 1:
        return np.unique(data[target_attribute_name])[0]
    elif len(data)==0:
        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]
    elif len(features) ==0:
        return parent_node_class
    else:
        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]
        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset
        best_feature_index = np.argmax(item_values)
        best_feature = features[best_feature_index]
        tree = {best_feature:{}}
        features = [i for i in features if i != best_feature]
        for value in np.unique(data[best_feature]):
            value = value
            sub_data = data.drop(data[data[best_feature]==value].index)
            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)
            tree[best_feature][value] = subtree
        return(tree) 
def predict(query,tree,default = 1):
    for key in list(query.keys()):
        if key in list(tree.keys()):
            try:
                result = tree[key][query[key]] 
            except:
                return default
            result = tree[key][query[key]]
            if isinstance(result,dict):
                return predict(query,result)
            else:
                return result
def train_test_split(dataset):
    training_data = dataset.iloc[:80].reset_index(drop=True)
    testing_data = dataset.iloc[80:].reset_index(drop=True)
    return training_data,testing_data
training_data = train_test_split(dataset)[0]
testing_data = train_test_split(dataset)[1] 
tree = ID3(dataset,dataset,dataset.columns[:-1])
pprint(tree)
</pre><br/>


<!--========================== FP GROWTH ======================== --> 
<pre  id="p3">
import pandas as pd

class treeNode:
    
    def __init__(self, value, numOccur, parent):
        self.name = value
        self.count = numOccur
        self.parentNode = parent
        self.link = None
        self.children = {}
        
    def inc(self, numOccur):
        self.count += numOccur
    
    def display(self, index = 1):
        print(" "*index, self.name, " : ", self.count)
        for child in self.children.values():
            child.display(index+1)

def updateHeader(headerItem, itemAdding):
    
    while(headerItem.link != None):
        headerItem = headerItem.link
    headerItem.link = itemAdding

def updateTree(orderedItems, tree, headerTable, count):
    
    if orderedItems[0] in tree.children:
        tree.children[orderedItems[0]].inc(count)
        
    else:
        tree.children[orderedItems[0]] = treeNode(orderedItems[0], count, tree)
        
        if headerTable[orderedItems[0]][1] == None:
            headerTable[orderedItems[0]][1] = tree.children[orderedItems[0]]
        else:
            updateHeader(headerTable[orderedItems[0]][1], tree.children[orderedItems[0]])
    
    if len(orderedItems) >1:
        updateTree(orderedItems[1::], tree.children[orderedItems[0]], headerTable, count)
        

def createFPTree(dictdata, minSupport):
    
    headerTable = {}
    for transaction in dictdata:
        for item in transaction:
            headerTable[item] = headerTable.get( item, 0 ) + dictdata[transaction]
    print(headerTable)
            
    #removing items with value less than minSupport
    for item in list(headerTable):
        if headerTable[item]<minSupport:
            del(headerTable[item])
            
    freqItemset = set(headerTable.keys())
    print('freqItemset:', freqItemset)
    
    if len(freqItemset) == 0:
        return None
    
    for item in headerTable:
        headerTable[item] = [headerTable[item], None]
        #print(headerTable[item])
    
    tree = treeNode("Nullset", 1, None)
    
    for transaction, count in dictdata.items():
        localdict = {}
        for item in transaction:
            if item in freqItemset:
                localdict[item] = headerTable[item][0]
        #print("localdict \n", localdict)
        if len(localdict) > 0:
            orderedItems = []
            for v in sorted(localdict.items(), key =lambda p:p[1], reverse = True):
                orderedItems.append(v[0])
            updateTree(orderedItems, tree, headerTable, count)
        #print("orderedItems \n", orderedItems)
    return tree, headerTable, freqItemset

def getdataset(filename):
    
    df = pd.read_csv(filename)
    dataset = []
    
    for index, rows in df.iterrows():
        row = []
        for item in rows:
            row.append(item)
        dataset.append(row)
        
    return dataset


def createFrozenset(dataset):
    
    dictionary = {}
    for item in dataset:
        dictionary[frozenset(item)] = 1
        
    return dictionary

filepath = "/DWDM/dataset.csv"
df = getdataset(filepath)
#print(df)

dictdata = createFrozenset(df)
#print(dictdata)

fpTree, headerTable, freqItemset = createFPTree(dictdata, 60)
fpTree.display()

def ascendTree(prefixPath, leafNode):
    if leafNode.parentNode != None:
        prefixPath.append(leafNode.name)
        ascendTree(prefixPath, leafNode.parentNode)

def findPrefixPath(item, treeNode):
    conditionalPatterns = {}
    
    while(treeNode!=None):
        prefixPath = []
        ascendTree(prefixPath, treeNode)
        
        if len(prefixPath) >1:
            conditionalPatterns[frozenset(prefixPath)] = treeNode.count
            
        treeNode = treeNode.link
    
    return conditionalPatterns
        
for item in freqItemset:
    x = findPrefixPath(item, headerTable[item][1])
    print(x)
</pre>

<!--========================== K MEANS ======================== -->
<pre id="p4">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
class K_means:
    def __init__(self, k=3, threshold=0.00001, max_iteration=500):
        self.k=k
        self.threshold=threshold
        self.max_iteration=max_iteration
    
    def fit(self, data): 
        self.centroids = {}
        for i in range(self.k):
            self.centroids[i]=data[i]
        for i in range(self.max_iteration):
            self.classes = {}
            for i in range(self.k): 
                self.classes[i] = []
            for feature in data:
                distances = [np.linalg.norm(feature-self.centroids[centroid]) for centroid in self.centroids]
                classification = distances.index(min(distances))
                self.classes[classification].append(feature)
            self.isTrue=True
            prev_centroids = dict(self.centroids)
            for i in range(self.k):
                self.centroids[i] = np.average(self.classes[i], axis=0)
            for i in range(self.k):
                if np.sum((self.centroids[i]-prev_centroids[i])/prev_centroids[i]) > self.threshold:
                    isTrue = False
            if isTrue==True:
                break
            
    def pred(self, data):
        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]
        classification = distances.index(min(distances))
        return classification
def main():
    df = pd.read_csv(r"./data/ipl.csv")
    df = df[['one','two']]
    #dataset = df.astype(float).values.tolist()
    X = df.values
    km = K_means(3)
    km.fit(X)
    
    colors = ['r', 'c', 'g', 'b', 'y']
    markers = ['+', 'x','*','<','>']
    
    for centroid in km.centroids:
        plt.scatter(km.centroids[centroid][0], km.centroids[centroid][1], color="black", s = 150, marker = "x")
        plt.scatter(km.centroids[centroid][0], km.centroids[centroid][1], color="black", marker="o", s=50)

    for classes in km.classes:
        color = colors[classes]
        marker = markers[classes]
        for data in km.classes[classes]:
            plt.scatter(data[0], data[1], color=color, marker=marker, s=50)
    
    pred=km.pred([0.5,0.5])
    print("Cluster: {0}  Color: {1}".format(pred+1,colors[pred]))
    print()
    plt.scatter(0.5, 0.5, color="purple", marker="^", s=80)
            
if __name__ == "__main__":
    main()
</pre><br/>


<!--========================== Naive bayes ======================== -->
<pre id="p5">
import csv
import random
import math
import pandas as pd
''' defining functions for individual tasks...
1. To open iris CSV file as dataframe
2. For splitting given dataset into taining and testing set( based on given split ratio)
3. Divide dataset into dictionary based on class labels. '''
def loadCSV(csv_filepath):
    dataframe = pd.read_csv(csv_filepath)
    #dataframe is like 2D matrix. If any data is non numeric, convert it into numeric one's.
    #Here in iris dataset, target variable,"variety" is non numeric. hence , need to be converted.
    num = {'Setosa':0,'Versicolor':1,'Virginica':2}
    dataframe = dataframe.replace(num)
    dataset=[]
    for index,rows in dataframe.iterrows():
        row = []
        for item in rows:
            row.append(item)
        dataset.append(row)
    
    return dataset
def trainTestSplit(dataset, splitratio):
    training_rows = int(splitratio*len(dataset))
    train = dataset[ : training_rows]
    test = dataset[training_rows : ]
    return train,test
def divideIntoDict(dataset):
    dict={}
    for row in dataset:
        if row[-1] not in dict:
            dict[row[-1]] = []
        dict[row[-1]].append(row)
    return dict
'''!!!!!!!!!     The above is a dictionary holding rows for each class label         !!!!!!!!!!!!!!!
This Naive bayes is000000 based on calculating probabilities.For discrete values like in play tennis dataset, it's fine.
 What about continuous values as attributes like in iris dataset?
Hence use gaussian probability density function to decide probabilities for each class variable.
 Follow the tuto link---> https://www.saedsayad.com/naive_bayesian.htm '''
def mean(numbers):
    return sum(numbers)/float(len(numbers))
 
def stdev(numbers):
    avg = mean(numbers)
    variance = 0
    for item in numbers:
        variance += (item-avg)*(item-avg)
    variance = variance/float(len(numbers)-1)
    return math.sqrt(variance)
''' Importance of zip function in python here
 x = zip([1,2],[3,4])
print(tuple(x))
o/p : ((1,3),(2,4))
 Here dict[0] is a list of lists. To extract all the lists at a time, use *dict[0]
summary_dict contains key as target and values as list of tuples, where each tuple corresponds to mean and std dev of an attribute   '''
def get_summary_dict(dict):
    summary_dict = {}
    for key in dict:
        summary_dict[key] = []
        for item in tuple(zip(*dict[key])):
            summary_dict[key].append((mean(item),stdev(item)))
        del summary_dict[key][-1]
    return summary_dict
# Gaussian Probability density function
def calculateProbability(x, mean, stdev):
    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))
    val=(1 / (math.sqrt(2*math.pi) * stdev)) * exponent
    return val
def calculateClassProbabilities(summary_dict,input_vector):
    probabilities = {}
    for key in summary_dict:
        probabilities[key] = 1
        i = 0
        for item in summary_dict[key]:
            x = input_vector[i] 
            probabilities[key] *= calculateProbability(x, item[0], item[1])
            i+=1
    return probabilities            
'''The above probabilities dictionary comprises of key as class and probability of tuple belongs to that class as value.
The input tuple belongs to the class of highest probability '''
def predict_class_label(summary_dict,input_vector):
    probabilities = calculateClassProbabilities(summary_dict,input_vector)
    highest_prob = 0
    for key in probabilities:
        if(probabilities[key]) >= highest_prob:
            label = key
            highest_prob = probabilities[key]
    return label
def getPredictions(summary_dict,test):
    predictions = []
    for row in test:
        result = predict_class_label(summary_dict,row)
        predictions.append(result)
    return predictions
 
def getAccuracy(test, predictions):
    correct = 0
    i=0
    for row in test:
        if row[-1] == predictions[i]:
            correct += 1
        i+=1
    return (correct/float(len(test))) * 100.0

def main():
    filepath = "dataset/NaiveBayes.csv"
    split_ratio = 0.68
    dataset = loadCSV(filepath)
    train,test = trainTestSplit(dataset,split_ratio)
    dict = divideIntoDict(train)
    summary_dict = get_summary_dict(dict)
    predictions = getPredictions(summary_dict,test)
    accuracy = getAccuracy(test, predictions)
    print(accuracy)
    #label=predict_class_label(summary_dict,[5.1,2.5,3,1.1])
    #print(label)

#Calling main driver function
main()    
</pre>
</center>
</body>
</html>
