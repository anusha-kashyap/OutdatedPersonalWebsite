<html>
<head>
<style type="text/css">
p, pre
{
display:none;
 color:white;
  font-size:20px;
}

button
{
  height:50px;
  width:50px;
 }
</style>
<script type="text/javascript">
function copyToClipboard(element) {
  var $temp = $("<input>");
  $("body").append($temp);
  $temp.val($(element).text()).select();
  document.execCommand("copy");
  $temp.remove();
}
</script>
</head>
<body>
<link href='https://fonts.googleapis.com/css?family=Oswald' rel='stylesheet' type='text/css'> 

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>


<button onclick="copyToClipboard('#p1')">A</button>

<button onclick="copyToClipboard('#p2')">D</button>

<button onclick="copyToClipboard('#p3')">F</button>

<button onclick="copyToClipboard('#p4')">K</button>

<button onclick="copyToClipboard('#p5')">N</button>
 

       
<center>

<!--========================== APRIORI ======================== -->
<pre id="p1">
import collections
def Apriori_gen(Itemset, lenght):
    """Too generate new (k+1)-itemsets can see README Join Stage"""
    canditate = []
    for i in range (0,lenght):
        element = str(Itemset[i])
        for j in range (i+1,lenght):
            element1 = str(Itemset[j])
            if element[0:(len(element)-1)] == element1[0:(len(element1)-1)]:
                    unionset = element[0:(len(element)-1)]+element1[len(element1)-1]+element[len(element)-1] #Combine (k-1)-Itemset to k-Itemset 
                    unionset = ''.join(sorted(unionset))  #Sort itemset by dict order
                    canditate.append(unionset)
    return canditate

def Apriori_prune(Ck,MinSupport):
    L = []
    for i in Ck:
        if Ck[i] >= minsupport:
            L.append(i)
    return sorted(L)

def Apriori_count_subset(Canditate,Canditate_len):
    """ Use bool to know is subset or not """
    Lk = dict()
    file = open('./dataset/apriori.csv')
    for l in file:
        l = str(l.split())
        for i in range (0,Canditate_len):
            key = str(Canditate[i])
            if key not in Lk:
                Lk[key] = 0
            flag = True
            for k in key:
                if k not in l:
                    flag = False
            if flag:
                Lk[key] += 1
    file.close()
    return Lk


minsupport = 3
print("Min Support is {}".format(minsupport))
C1={} 
file = open('./dataset/apriori.csv')
"""Count one canditate"""
for line in file:
    print(line)
    for item in line.split():
        if item in C1:
            C1[item] +=1
        else:
            C1[item] = 1
file.close()
C1 = collections.OrderedDict(sorted(C1.items()))
L = []
L1 = Apriori_prune(C1,minsupport)
L = Apriori_gen(L1,len(L1))
</pre>
 
 
 <!--========================== DECISION TREE ======================== -->
<pre id="p2">
import pandas as pd
import numpy as np
from pprint import pprint
dataset = pd.read_csv('dataset/decision_tree.csv',names=['outlook','temp','humidity','windy','play_golf',])
def entropy(target_col):
    elements,counts = np.unique(target_col,return_counts = True)
    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])
    return entropy
def InfoGain(data,split_attribute_name,target_name="play_golf"):   
    total_entropy = entropy(data[target_name]) 
    vals,counts= np.unique(data[split_attribute_name],return_counts=True)
    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])
    Information_Gain = total_entropy - Weighted_Entropy
    return Information_Gain       
def ID3(data,originaldata,features,target_attribute_name="play_golf",parent_node_class = None):
    if len(np.unique(data[target_attribute_name])) == 1:
        return np.unique(data[target_attribute_name])[0]
    elif len(data)==0:
        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]
    elif len(features) ==0:
        return parent_node_class
    else:
        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]
        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset
        best_feature_index = np.argmax(item_values)
        best_feature = features[best_feature_index]
        tree = {best_feature:{}}
        features = [i for i in features if i != best_feature]
        for value in np.unique(data[best_feature]):
            value = value
            sub_data = data.drop(data[data[best_feature]==value].index)
            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)
            tree[best_feature][value] = subtree
        return(tree) 
def predict(query,tree,default = 1):
    for key in list(query.keys()):
        if key in list(tree.keys()):
            try:
                result = tree[key][query[key]] 
            except:
                return default
            result = tree[key][query[key]]
            if isinstance(result,dict):
                return predict(query,result)
            else:
                return result
def train_test_split(dataset):
    training_data = dataset.iloc[:80].reset_index(drop=True)
    testing_data = dataset.iloc[80:].reset_index(drop=True)
    return training_data,testing_data
training_data = train_test_split(dataset)[0]
testing_data = train_test_split(dataset)[1] 
tree = ID3(dataset,dataset,dataset.columns[:-1])
pprint(tree)
</pre><br/>


<!--========================== FP GROWTH ======================== --> 
<pre  id="p3">
import time

#Function to load file and return lists of Transactions
def Load_data(filename):
    with open(filename) as f:
        content = f.readlines()

    content = [x.strip() for x in content]
    Transaction = []

    for i in range(0, len(content)):
        Transaction.append(content[i].split())

    return Transaction

#To convert initial transaction into frozenset
def create_initialset(dataset):
    retDict = {}
    for trans in dataset:
        retDict[frozenset(trans)] = 1
    return retDict

#class of FP TREE node
class TreeNode:
    def __init__(self, Node_name,counter,parentNode):
        self.name = Node_name
        self.count = counter
        self.nodeLink = None
        self.parent = parentNode
        self.children = {}
        
    def increment_counter(self, counter):
        self.count += counter

#To create Headertable and ordered itemsets for FP Tree
def create_FPTree(dataset, minSupport):
    HeaderTable = {}
    for transaction in dataset:
        for item in transaction:
            HeaderTable[item] = HeaderTable.get(item,0) + dataset[transaction]
    for k in list(HeaderTable):
        if HeaderTable[k] < minSupport:
            del(HeaderTable[k])

    frequent_itemset = set(HeaderTable.keys())

    if len(frequent_itemset) == 0:
        return None, None

    for k in HeaderTable:
        HeaderTable[k] = [HeaderTable[k], None]

    retTree = TreeNode('Null Set',1,None)
    for itemset,count in dataset.items():
        frequent_transaction = {}
        for item in itemset:
            if item in frequent_itemset:
                frequent_transaction[item] = HeaderTable[item][0]
        if len(frequent_transaction) > 0:
            #to get ordered itemsets form transactions
            ordered_itemset = [v[0] for v in sorted(frequent_transaction.items(), key=lambda p: p[1], reverse=True)]
            #to update the FPTree
            updateTree(ordered_itemset, retTree, HeaderTable, count)
    return retTree, HeaderTable

#To create the FP Tree using ordered itemsets
def updateTree(itemset, FPTree, HeaderTable, count):
    if itemset[0] in FPTree.children:
        FPTree.children[itemset[0]].increment_counter(count)
    else:
        FPTree.children[itemset[0]] = TreeNode(itemset[0], count, FPTree)

        if HeaderTable[itemset[0]][1] == None:
            HeaderTable[itemset[0]][1] = FPTree.children[itemset[0]]
        else:
            update_NodeLink(HeaderTable[itemset[0]][1], FPTree.children[itemset[0]])

    if len(itemset) > 1:
        updateTree(itemset[1::], FPTree.children[itemset[0]], HeaderTable, count)

#To update the link of node in FP Tree
def update_NodeLink(Test_Node, Target_Node):
    while (Test_Node.nodeLink != None):
        Test_Node = Test_Node.nodeLink

    Test_Node.nodeLink = Target_Node

#To transverse FPTree in upward direction
def FPTree_uptransveral(leaf_Node, prefixPath):
 if leaf_Node.parent != None:
    prefixPath.append(leaf_Node.name)
    FPTree_uptransveral(leaf_Node.parent, prefixPath)

#To find conditional Pattern Bases
def find_prefix_path(basePat, TreeNode):
 Conditional_patterns_base = {}

 while TreeNode != None:
    prefixPath = []
    FPTree_uptransveral(TreeNode, prefixPath)
    if len(prefixPath) > 1:
        Conditional_patterns_base[frozenset(prefixPath[1:])] = TreeNode.count
    TreeNode = TreeNode.nodeLink

 return Conditional_patterns_base

#function to mine recursively conditional patterns base and conditional FP tree
def Mine_Tree(FPTree, HeaderTable, minSupport, prefix, frequent_itemset):
    bigL = [v[0] for v in sorted(HeaderTable.items(),key=lambda p: p[1][0])]
    for basePat in bigL:
        new_frequentset = prefix.copy()
        new_frequentset.add(basePat)
        #add frequent itemset to final list of frequent itemsets
        frequent_itemset.append(new_frequentset)
        #get all conditional pattern bases for item or itemsets
        Conditional_pattern_bases = find_prefix_path(basePat, HeaderTable[basePat][1])
        #call FP Tree construction to make conditional FP Tree
        Conditional_FPTree, Conditional_header = create_FPTree(Conditional_pattern_bases,minSupport)

        if Conditional_header != None:
            Mine_Tree(Conditional_FPTree, Conditional_header, minSupport, new_frequentset, frequent_itemset)

#to take input of filename and minimum support
print("Enter the filename:")
filename = input()
print("Enter the minimum support count:")
min_Support = int(input())

initSet = create_initialset(Load_data(filename))
start = time.time()
FPtree, HeaderTable = create_FPTree(initSet, min_Support)

frequent_itemset = []
#call function to mine all ferquent itemsets
Mine_Tree(FPtree, HeaderTable, min_Support, set([]), frequent_itemset)
end = time.time()

print("Time Taken is:")
print(end-start)
print("All frequent itemsets:")
print(frequent_itemset)
</pre>

<!--========================== K MEANS ======================== -->
<pre id="p4">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
class K_means:
    def __init__(self, k=3, threshold=0.00001, max_iteration=500):
        self.k=k
        self.threshold=threshold
        self.max_iteration=max_iteration
    
    def fit(self, data): 
        self.centroids = {}
        for i in range(self.k):
            self.centroids[i]=data[i]
        for i in range(self.max_iteration):
            self.classes = {}
            for i in range(self.k): 
                self.classes[i] = []
            for feature in data:
                distances = [np.linalg.norm(feature-self.centroids[centroid]) for centroid in self.centroids]
                classification = distances.index(min(distances))
                self.classes[classification].append(feature)
            self.isTrue=True
            prev_centroids = dict(self.centroids)
            for i in range(self.k):
                self.centroids[i] = np.average(self.classes[i], axis=0)
            for i in range(self.k):
                if np.sum((self.centroids[i]-prev_centroids[i])/prev_centroids[i]) > self.threshold:
                    isTrue = False
            if isTrue==True:
                break
            
    def pred(self, data):
        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]
        classification = distances.index(min(distances))
        return classification
def main():
    df = pd.read_csv(r"./data/ipl.csv")
    df = df[['one','two']]
    #dataset = df.astype(float).values.tolist()
    X = df.values
    km = K_means(3)
    km.fit(X)
    
    colors = ['r', 'c', 'g', 'b', 'y']
    markers = ['+', 'x','*','<','>']
    
    for centroid in km.centroids:
        plt.scatter(km.centroids[centroid][0], km.centroids[centroid][1], color="black", s = 150, marker = "x")
        plt.scatter(km.centroids[centroid][0], km.centroids[centroid][1], color="black", marker="o", s=50)

    for classes in km.classes:
        color = colors[classes]
        marker = markers[classes]
        for data in km.classes[classes]:
            plt.scatter(data[0], data[1], color=color, marker=marker, s=50)
    
    pred=km.pred([0.5,0.5])
    print("Cluster: {0}  Color: {1}".format(pred+1,colors[pred]))
    print()
    plt.scatter(0.5, 0.5, color="purple", marker="^", s=80)
            
if __name__ == "__main__":
    main()
</pre><br/>


<!--========================== Naive bayes ======================== -->
<pre id="p5">
import csv
import random
import math
import pandas as pd
''' defining functions for individual tasks...
1. To open iris CSV file as dataframe
2. For splitting given dataset into taining and testing set( based on given split ratio)
3. Divide dataset into dictionary based on class labels. '''
def loadCSV(csv_filepath):
    dataframe = pd.read_csv(csv_filepath)
    #dataframe is like 2D matrix. If any data is non numeric, convert it into numeric one's.
    #Here in iris dataset, target variable,"variety" is non numeric. hence , need to be converted.
    num = {'Setosa':0,'Versicolor':1,'Virginica':2}
    dataframe = dataframe.replace(num)
    dataset=[]
    for index,rows in dataframe.iterrows():
        row = []
        for item in rows:
            row.append(item)
        dataset.append(row)
    
    return dataset
def trainTestSplit(dataset, splitratio):
    training_rows = int(splitratio*len(dataset))
    train = dataset[ : training_rows]
    test = dataset[training_rows : ]
    return train,test
def divideIntoDict(dataset):
    dict={}
    for row in dataset:
        if row[-1] not in dict:
            dict[row[-1]] = []
        dict[row[-1]].append(row)
    return dict
'''!!!!!!!!!     The above is a dictionary holding rows for each class label         !!!!!!!!!!!!!!!
This Naive bayes is000000 based on calculating probabilities.For discrete values like in play tennis dataset, it's fine.
 What about continuous values as attributes like in iris dataset?
Hence use gaussian probability density function to decide probabilities for each class variable.
 Follow the tuto link---> https://www.saedsayad.com/naive_bayesian.htm '''
def mean(numbers):
    return sum(numbers)/float(len(numbers))
 
def stdev(numbers):
    avg = mean(numbers)
    variance = 0
    for item in numbers:
        variance += (item-avg)*(item-avg)
    variance = variance/float(len(numbers)-1)
    return math.sqrt(variance)
''' Importance of zip function in python here
 x = zip([1,2],[3,4])
print(tuple(x))
o/p : ((1,3),(2,4))
 Here dict[0] is a list of lists. To extract all the lists at a time, use *dict[0]
summary_dict contains key as target and values as list of tuples, where each tuple corresponds to mean and std dev of an attribute   '''
def get_summary_dict(dict):
    summary_dict = {}
    for key in dict:
        summary_dict[key] = []
        for item in tuple(zip(*dict[key])):
            summary_dict[key].append((mean(item),stdev(item)))
        del summary_dict[key][-1]
    return summary_dict
# Gaussian Probability density function
def calculateProbability(x, mean, stdev):
    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))
    val=(1 / (math.sqrt(2*math.pi) * stdev)) * exponent
    return val
def calculateClassProbabilities(summary_dict,input_vector):
    probabilities = {}
    for key in summary_dict:
        probabilities[key] = 1
        i = 0
        for item in summary_dict[key]:
            x = input_vector[i] 
            probabilities[key] *= calculateProbability(x, item[0], item[1])
            i+=1
    return probabilities            
'''The above probabilities dictionary comprises of key as class and probability of tuple belongs to that class as value.
The input tuple belongs to the class of highest probability '''
def predict_class_label(summary_dict,input_vector):
    probabilities = calculateClassProbabilities(summary_dict,input_vector)
    highest_prob = 0
    for key in probabilities:
        if(probabilities[key]) >= highest_prob:
            label = key
            highest_prob = probabilities[key]
    return label
def getPredictions(summary_dict,test):
    predictions = []
    for row in test:
        result = predict_class_label(summary_dict,row)
        predictions.append(result)
    return predictions
 
def getAccuracy(test, predictions):
    correct = 0
    i=0
    for row in test:
        if row[-1] == predictions[i]:
            correct += 1
        i+=1
    return (correct/float(len(test))) * 100.0

def main():
    filepath = "dataset/NaiveBayes.csv"
    split_ratio = 0.68
    dataset = loadCSV(filepath)
    train,test = trainTestSplit(dataset,split_ratio)
    dict = divideIntoDict(train)
    summary_dict = get_summary_dict(dict)
    predictions = getPredictions(summary_dict,test)
    accuracy = getAccuracy(test, predictions)
    print(accuracy)
    #label=predict_class_label(summary_dict,[5.1,2.5,3,1.1])
    #print(label)

#Calling main driver function
main()    
</pre>
</center>
</body>
</html>
